{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Faster_R_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62ad27ced64948018269bcb861dcb6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a80684b599f402d96cee66b8557b20b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_26a72f79058e4d43b8bf516b00918b45",
              "IPY_MODEL_5822ce01016d437d9c42b15886b04425"
            ]
          }
        },
        "5a80684b599f402d96cee66b8557b20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26a72f79058e4d43b8bf516b00918b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9df443915b8b41d2afba52d37096af47",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 167502836,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 167502836,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_63c78dc882194af5a3d25d436bbac179"
          }
        },
        "5822ce01016d437d9c42b15886b04425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3503869b20c748a28e2f79f05c0ace99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 160M/160M [00:53&lt;00:00, 3.14MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3dc4264695374fbeb9f01c3b4acb14af"
          }
        },
        "9df443915b8b41d2afba52d37096af47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "63c78dc882194af5a3d25d436bbac179": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3503869b20c748a28e2f79f05c0ace99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3dc4264695374fbeb9f01c3b4acb14af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__fdHMvV9qAR"
      },
      "source": [
        "# **Addestramento di una rete neurale Faster R-CNN per la detection di luoghi ad alto valore culturale**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnbbKFIIhFcD"
      },
      "source": [
        "Pezzo da aggiungere a /usr/local/lib/python3.6/dist-packages/albumentations/augmentations/bbox_utils.py\n",
        "\n",
        "```\n",
        "    bbox=list(bbox)\n",
        "\n",
        "    for i in range(4):\n",
        "      if (bbox[i]<0) :\n",
        "        bbox[i]=0\n",
        "      elif (bbox[i]>1) :\n",
        "        bbox[i]=1\n",
        "\n",
        "    bbox=tuple(bbox)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtkV3lac9USc"
      },
      "source": [
        "## Librerie\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYSpeDCBVhqg"
      },
      "source": [
        "Clonazione del repository *vision* di *pytorch* per utilizzare alcune funzioni di utilitÃ  per l'addestramento e valutazione del modello."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6_xR-Xh23yE",
        "outputId": "4404a01a-1225-4a58-999b-d4f77eedc286"
      },
      "source": [
        "%%shell\n",
        "\n",
        "git clone https://github.com/pytorch/vision.git\n",
        "cd vision\n",
        "git checkout v0.3.0       # Check current version\n",
        "\n",
        "cp references/detection/utils.py ../\n",
        "cp references/detection/transforms.py ../\n",
        "cp references/detection/engine.py ../\n",
        "cp references/detection/coco_utils.py ../\n",
        "cp references/detection/coco_eval.py ../"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'vision'...\n",
            "remote: Enumerating objects: 33155, done.\u001b[K\n",
            "remote: Counting objects: 100% (5698/5698), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1246/1246), done.\u001b[K\n",
            "remote: Total 33155 (delta 4529), reused 5437 (delta 4338), pack-reused 27457\u001b[K\n",
            "Receiving objects: 100% (33155/33155), 40.47 MiB | 23.23 MiB/s, done.\n",
            "Resolving deltas: 100% (25145/25145), done.\n",
            "Note: checking out 'v0.3.0'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at be376084 version check against PyTorch's CUDA version\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              ""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hy8Go5h4Pp_b"
      },
      "source": [
        "Installazione della libreria albumentations con specifica versione 0.4.6.\n",
        "\n",
        "\n",
        "Questa libreria verrÃ  utilizzata per eseguire operazioni di trasformazione di *data augmentation* sui dati. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mc2l_5uePaaj",
        "outputId": "ddd9506f-31cb-4dee-ed59-357d667aaed5"
      },
      "source": [
        "! pip install albumentations==0.4.6"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 117 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 948 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.16.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.7.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.5.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.3.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (4.4.2)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65173 sha256=7764673dbf46dd6859e78e4d161b718d7cbf9178dca6c24046117459d8186ccf\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1Yj9r3O3RMW"
      },
      "source": [
        "Librerie da importare:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8yOZMzd950I"
      },
      "source": [
        "# Core libraries\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "import numpy as np\n",
        "import random\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, Subset\n",
        "import transforms as T                          # Module transform  (helper functions)\n",
        "import utils                                    # Module utils      (helper functions)\n",
        "from engine import train_one_epoch, evaluate    # Module engine     (helper functions)\n",
        "\n",
        "# Data Augmentation\n",
        "import albumentations as A\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "# Opencv\n",
        "import cv2 as cv\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh3UYPXk-Ta4"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "Sono stati individuati **24 punti di interesse** suddivisi in altrettante directories. \n",
        "\n",
        "\n",
        "\n",
        "Ogni punto di interesse contiene, a sua volta, altre tre directory:\n",
        "- *imgs*: contiene le immagini di dimensione 900 x 1300;\n",
        "- *annotations*: contiene le annotazioni (coordinate delle bounding box) di ogni immagine catturata;\n",
        "- *bounding_box*: contiene le immagini con le bouding box individuate.\n",
        "\n",
        "\n",
        "Il dataset Ã¨ consultabile al seguente [link](https://drive.google.com/drive/folders/1MenVMP6C9Vuo7JJ-iesAd1KfsemAvSLy?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2W6j3983Ewsn"
      },
      "source": [
        "### Connessione a Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYhwFJpva4og"
      },
      "source": [
        "Connessione a google drive per scaricare il dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENHAJClM-Xas",
        "outputId": "617e8ee0-4a6b-42e9-9ca8-52523b8b4d1c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "path = \"gdrive/MyDrive/DataSet/DatasetMonuments/\"   # Replace with your path"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qhZlG39Br1u"
      },
      "source": [
        "### Caricamento del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEr5mIK_NE-M"
      },
      "source": [
        "La funzione *get_bounding_boxes* permette di ottenere le coordinate di tutte le bouding boxes di uno specifico file .xml dato in input.\n",
        "\n",
        "La funzione *format_labels* permette semplicemente di far 'matchare' le labels ottenute dal nome delle loro directories con quelle memorizzate nel file .xml (annotazione) di una specifica immagine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9z3U0FSNPtd"
      },
      "source": [
        "def get_bounding_boxes(file_xml):\n",
        "  r\"\"\"\n",
        "  Get all bounding boxes of a specific file xml.\n",
        "  :param file_xml: file from which to get bounding boxes\n",
        "  \"\"\"\n",
        "\n",
        "  bounding_box_list = []\n",
        "  labels = []\n",
        "\n",
        "  tree = ET.parse(file_xml)\n",
        "  root = tree.getroot()\n",
        "\n",
        "  # Get all bounding boxes\n",
        "  j = 6\n",
        "  while True:\n",
        "      try:\n",
        "          object = root[j]\n",
        "\n",
        "          labels.append(object[0].text)\n",
        "\n",
        "          box = object[4]\n",
        "          x_min, y_min, x_max, y_max = int(box[0].text), int(box[1].text), int(box[2].text), int(box[3].text)\n",
        "          bounding_box_list.append([[x_min, y_min], [x_max, y_max]])\n",
        "\n",
        "          j += 1\n",
        "          \n",
        "      except:\n",
        "          break\n",
        "\n",
        "  return bounding_box_list, format_labels(labels)\n",
        "\n",
        "def format_labels(list_labels):\n",
        "  r\"\"\"\n",
        "  Match the labels.\n",
        "  :param list_label: list containing the labels to math.\n",
        "  \"\"\"\n",
        "\n",
        "  new_list = []\n",
        "  for label in list_labels:\n",
        "\n",
        "    if label == \"Centro Arti Visive Pescheria\":\n",
        "      new_list.append(\"Centro Arti Visive\")\n",
        "\n",
        "    elif label == \"Giuseppe Garibaldi\":\n",
        "      new_list.append(\"Statua Giuseppe Garibaldi\")\n",
        "\n",
        "    elif label == \"Scultura della Memoria\":\n",
        "      new_list.append(\"Scultura Della Memoria\")\n",
        "\n",
        "    elif label == \"Giulio Perticari\":\n",
        "      new_list.append(\"Statua Giulio Perticari\")\n",
        "\n",
        "    elif label == \"Gioachino Rossini\":\n",
        "      new_list.append(\"Statua Gioachino Rossini\")\n",
        "      \n",
        "    elif label == \"Palla di Pomodoro\":\n",
        "      new_list.append(\"Palla Di Pomodoro\")\n",
        "\n",
        "    elif label == \"Arco di Trionfo\":\n",
        "      new_list.append(\"Arco Di Trionfo\")\n",
        "\n",
        "    elif label == \"Palazzo delle Poste\":\n",
        "      new_list.append(\"Palazzo Delle Poste\")\n",
        "\n",
        "    elif label == \"Fontana Piazza del Popolo\":\n",
        "      new_list.append(\"Fontana Piazza\")\n",
        "      \n",
        "    else:\n",
        "      new_list.append(label)\n",
        "  \n",
        "  return new_list"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGXkte6hR4Xt"
      },
      "source": [
        "La funzione *get_transform* applica trasformazioni di *data augmentation* prima di darli in input alla rete."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU6ypVtUR4cj"
      },
      "source": [
        "def get_transform(train):\n",
        "  r\"\"\"\n",
        "  Apply the transformations.\n",
        "  :param: true if it is need to apply the horizontal flip, false otherwise.\n",
        "  \"\"\"\n",
        "\n",
        "  if train:\n",
        "      return A.Compose([A.HorizontalFlip(0.5), ToTensorV2(p=1.0)],\n",
        "                       bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n",
        "  else:\n",
        "      return A.Compose([ToTensorV2(p=1.0)],\n",
        "                       bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnoOs9OZa9rX"
      },
      "source": [
        "Per ogni etichetta, verrÃ  creata un'istanza della classe *DatasetCulturalHeritage*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Dn4gqqNGLoS"
      },
      "source": [
        "class DatasetCulturalHeritage(Dataset):\n",
        "  r\"\"\"\n",
        "  Class to load the dataset of a specific label.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, label, label_ds, path, transforms=None, height=224, width=224):\n",
        "      r\"\"\"\n",
        "      Load the dataset.\n",
        "      :param label: label of the dataset.\n",
        "      :param label_ds: label of the dataset not formatted. \n",
        "      :param dir: directory of the monumnet/place.\n",
        "      :param transform: transform to apply to the data.\n",
        "      \"\"\"\n",
        "\n",
        "      self.label_name   = label\n",
        "      self.label_ds     = label_ds\n",
        "      self.img_list     = glob.glob(path + self.label_ds + '/imgs/*.JPG') + glob.glob(path + self.label_ds + '/imgs/*.jpg')\n",
        "      self.annotation_list = glob.glob(path + self.label_ds + '/annotations/*.xml')\n",
        "\n",
        "      self.img_list.sort()\n",
        "      self.annotation_list.sort()\n",
        "\n",
        "      self.transforms = transforms\n",
        "      self.height = height\n",
        "      self.width = width\n",
        "\n",
        "      print(f\"Classe [{self.label_name}] caricata!\\n\")\n",
        "    \n",
        "  def __getitem__(self, index):\n",
        "      r\"\"\"\n",
        "      Get the image and its bouding boxes.\n",
        "      :param index: index of the specific image\n",
        "      \"\"\"\n",
        "      global total_labels\n",
        "\n",
        "      img_path, annotations_path = self.img_list[index], self.annotation_list[index]\n",
        "\n",
        "      img = cv.imread(img_path)\n",
        "\n",
        "      height, width, _ = img.shape\n",
        "\n",
        "      img = cv.resize(img, (self.width, self.height), cv.INTER_AREA)\n",
        "      img = cv.cvtColor(img, cv.COLOR_BGR2RGB).astype(np.float32)\n",
        "      img /= 255.0\n",
        "\n",
        "      # Bouding boxes\n",
        "      bounding_boxes = []\n",
        "      labels = []\n",
        "      _boxes, _labels = get_bounding_boxes(annotations_path)\n",
        "\n",
        "      for box, label in zip(_boxes, _labels):\n",
        "\n",
        "        labels.append(total_labels.index(label))\n",
        "      \n",
        "        # Extracting scaled coordinates for the size of the current image\n",
        "        x_min = (box[0][0] / width) * self.width\n",
        "        x_max = (box[1][0] / width) * self.width\n",
        "        y_min = (box[0][1] / height)* self.height\n",
        "        y_max = (box[1][1] / height)* self.height\n",
        "\n",
        "        bounding_boxes.append([x_min, y_min, x_max, y_max])\n",
        "\n",
        "      if len(bounding_boxes) == 0:\n",
        "        # Image without bounding box\n",
        "        bb_tensor = torch.as_tensor([[0, 0, 0, 0]], dtype=torch.float32)\n",
        "\n",
        "      else:\n",
        "        # Image with bounding box\n",
        "        bb_tensor = torch.as_tensor(bounding_boxes, dtype=torch.float32)\n",
        "\n",
        "      # Labels\n",
        "      labels = torch.as_tensor(labels, dtype=torch.int64)\n",
        "\n",
        "      # Area\n",
        "      area = (bb_tensor[:, 3] - bb_tensor[:, 1]) * \\\n",
        "              (bb_tensor[:, 2] - bb_tensor[:, 0])\n",
        "\n",
        "      # Id\n",
        "      img_id = torch.tensor([index])\n",
        "\n",
        "      is_crowd = torch.zeros((len(bb_tensor),), dtype=torch.int64)\n",
        "\n",
        "      target = {}\n",
        "      target[\"boxes\"] = bb_tensor\n",
        "      target[\"labels\"] = labels\n",
        "      target[\"image_id\"] = img_id\n",
        "      target[\"area\"] = area\n",
        "      target[\"is_crowd\"] = is_crowd\n",
        "\n",
        "      if self.transforms is not None and len(bounding_boxes) > 0:\n",
        "\n",
        "        result_transform = self.transforms(image = img,\n",
        "                                           bboxes = target['boxes'],\n",
        "                                           labels = labels)\n",
        "        \n",
        "        img = result_transform['image']\n",
        "        target['boxes'] = torch.Tensor(result_transform['bboxes'])\n",
        "\n",
        "      return img, target\n",
        "\n",
        "  def length(self):\n",
        "      return len(self.img_list), len(self.annotation_list)\n",
        "\n",
        "  def __len__(self):\n",
        "      assert len(self.img_list) == len(self.annotation_list)\n",
        "\n",
        "      return len(self.img_list)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuXo-es_bP4_"
      },
      "source": [
        "La funzione *split_directory* permette di formattare il testo dato in input sulla base delle lettere maiuscole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1WKDZCpOYzm"
      },
      "source": [
        "def split_directory(title):\n",
        "    r\"\"\"\n",
        "    Split text of the directory\n",
        "    :param title: text to split\n",
        "    :return: text with the tabulations\n",
        "    \"\"\"\n",
        "    \n",
        "    title = title[0].upper() + title[1:]\n",
        "    folder = re.findall('[A-Z][^A-Z]*', title)\n",
        "    result = \"\"\n",
        "\n",
        "    for i, item in enumerate(folder):\n",
        "        result += str(item) + (\" \" if (i + 1) != len(folder) else \"\")\n",
        "\n",
        "    return result"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-FuSnszbj0x"
      },
      "source": [
        "Caricamento del dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "539-6kAPFfYR",
        "outputId": "34866e49-6450-4c1b-f424-5b7e1e67049e"
      },
      "source": [
        "global total_labels\n",
        "\n",
        "dataset = []\n",
        "num_imgs = 0\n",
        "num_annotations = 0\n",
        "total_labels = []\n",
        "\n",
        "print(\"Caricamento dataset in corso ...\\n\\n\")\n",
        "\n",
        "dirs = os.listdir(path)\n",
        "\n",
        "for index, dir in enumerate(dirs):\n",
        "\n",
        "  name_class = split_directory(dir)\n",
        "\n",
        "  class_label = DatasetCulturalHeritage(label=name_class, label_ds=dir, path=path, transforms=get_transform(train=True))\n",
        "\n",
        "  _num_imgs, _num_annotations = class_label.length()\n",
        "  num_imgs += _num_imgs\n",
        "  num_annotations += _num_annotations\n",
        "\n",
        "  total_labels.append(name_class)\n",
        "  dataset.append(class_label)\n",
        "\n",
        "print(\"Caricamento terminato.\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Caricamento dataset in corso ...\n",
            "\n",
            "\n",
            "Classe [Villino Ruggeri] caricata!\n",
            "\n",
            "Classe [Statua Gioachino Rossini] caricata!\n",
            "\n",
            "Classe [Scultura Della Memoria] caricata!\n",
            "\n",
            "Classe [Rocca Costanza] caricata!\n",
            "\n",
            "Classe [Portale San Domenico] caricata!\n",
            "\n",
            "Classe [Teatro Rossini] caricata!\n",
            "\n",
            "Classe [Statua Giulio Perticari] caricata!\n",
            "\n",
            "Classe [Statua Giuseppe Garibaldi] caricata!\n",
            "\n",
            "Classe [Teatro Sperimentale] caricata!\n",
            "\n",
            "Classe [Villa Caprile] caricata!\n",
            "\n",
            "Classe [Palla Di Pomodoro] caricata!\n",
            "\n",
            "Classe [Palazzo Ducale] caricata!\n",
            "\n",
            "Classe [Parrocchia Santa Maria] caricata!\n",
            "\n",
            "Classe [Palazzo Comunale] caricata!\n",
            "\n",
            "Classe [Palazzo Delle Poste] caricata!\n",
            "\n",
            "Classe [Palazzo Olivieri] caricata!\n",
            "\n",
            "Classe [Chiesa San Agostino] caricata!\n",
            "\n",
            "Classe [Fontana Piazza] caricata!\n",
            "\n",
            "Classe [Centro Arti Visive] caricata!\n",
            "\n",
            "Classe [Palazzo Baviera] caricata!\n",
            "\n",
            "Classe [Arco Di Trionfo] caricata!\n",
            "\n",
            "Classe [Casa Rossini] caricata!\n",
            "\n",
            "Classe [Berlina Mosca] caricata!\n",
            "\n",
            "Classe [Cattedrale Santa Maria Assunta] caricata!\n",
            "\n",
            "Classe [Free] caricata!\n",
            "\n",
            "Caricamento terminato.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opx-o9X5Oc73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "530dc4d8-afbc-4707-efbc-fb9ca379918d"
      },
      "source": [
        "print(f\"Numero totale di immagini: {num_imgs}\\n\")\n",
        "print(f\"Numero totale di annotazioni: {num_annotations}\", end=\"\\n\\n\")\n",
        "print(f\"Numero totale di classi: {len(total_labels)}\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Numero totale di immagini: 953\n",
            "\n",
            "Numero totale di annotazioni: 953\n",
            "\n",
            "Numero totale di classi: 25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjX7uIIROyLk"
      },
      "source": [
        "### Visualizzazione del dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgA52TK3aehC"
      },
      "source": [
        "La funzione *stack_images* permette di mostrare una lista orizzontale e/o verticale di immagini."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suyN_7QKO1QT"
      },
      "source": [
        "def stack_images(scale, img_array):\n",
        "    r\"\"\"\n",
        "    Stack the images based on the number of them by rows and columns.\n",
        "    Resize the images.\n",
        "    :param scale: scale factor.\n",
        "    :param img_array: array of images.\n",
        "    :return: array of images to show.\n",
        "    \"\"\"\n",
        "\n",
        "    rows = len(img_array)\n",
        "    cols = len(img_array[0])\n",
        "\n",
        "    rowsAvailable = isinstance(img_array[0], list)\n",
        "\n",
        "    width = img_array[0][0].shape[1]\n",
        "    height = img_array[0][0].shape[0]\n",
        "\n",
        "    if rowsAvailable:\n",
        "        for x in range(0, rows):\n",
        "            for y in range(0, cols):\n",
        "\n",
        "                if img_array[x][y].shape[:2] == img_array[0][0].shape[:2]:\n",
        "                    img_array[x][y] = cv.resize(img_array[x][y], (0, 0), None, scale, scale)\n",
        "                else:\n",
        "                    img_array[x][y] = cv.resize(img_array[x][y], (img_array[0][0].shape[1], img_array[0][0].shape[0]),\n",
        "                                                None, scale, scale)\n",
        "\n",
        "                if len(img_array[x][y].shape) == 2:\n",
        "                    img_array[x][y] = cv.cvtColor(img_array[x][y], cv.COLOR_GRAY2BGR)\n",
        "\n",
        "        imageBlank = np.zeros((height, width, 3), np.uint8)\n",
        "        hor = [imageBlank] * rows\n",
        "\n",
        "        for x in range(0, rows):\n",
        "            hor[x] = np.hstack(img_array[x])\n",
        "        ver = np.vstack(hor)\n",
        "\n",
        "    else:\n",
        "        for x in range(0, rows):\n",
        "\n",
        "            if img_array[x].shape[:2] == img_array[0].shape[:2]:\n",
        "                img_array[x] = cv.resize(img_array[x], (0, 0), None, scale, scale)\n",
        "            else:\n",
        "                img_array[x] = cv.resize(img_array[x], (img_array[0].shape[1], img_array[0].shape[0]), None, scale,\n",
        "                                         scale)\n",
        "\n",
        "            if len(img_array[x].shape) == 2:\n",
        "                img_array[x] = cv.cvtColor(img_array[x], cv.COLOR_GRAY2BGR)\n",
        "\n",
        "        hor = np.hstack(img_array)\n",
        "        ver = hor\n",
        "\n",
        "    return ver\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6uAtsvtc6Td"
      },
      "source": [
        "La funzione *draw_description* disegna il nome della classe in basso nell'immagine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvDnGO9AdEwE"
      },
      "source": [
        "def draw_description(img, text):\n",
        "    r\"\"\"\n",
        "    Draw description image.\n",
        "    :param text: image description\n",
        "    :param img: image in which to insert the descrition\n",
        "    :return: image with description\n",
        "    \"\"\"\n",
        "\n",
        "    bottom = int(0.04 * img.shape[0])\n",
        "    img = cv.copyMakeBorder(img, 0, bottom, 0, 0, cv.BORDER_CONSTANT, None, (255, 255, 255))\n",
        "\n",
        "    height, _, _ = img.shape\n",
        "    cv.putText(img, text, (0, height - 5), cv.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n",
        "    \n",
        "    return img"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rp8U8ZVmGWhZ"
      },
      "source": [
        "La funzione *convert_point_to_tuple* converte una coordinata (punto x,y) in una tupla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXRHmH_hEkvg"
      },
      "source": [
        "def convert_point_to_tuple(point):\n",
        "    x, y = point\n",
        "    return (x,y)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fb3ZiS9JlT9"
      },
      "source": [
        "Il codice seguente mostra una griglia di 4 immagini casuali prese dal dataset. Per ogni immagine, a sinistra, viene mostrata quella originale e a destra la stessa immagine con le bounding box individuate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V95ZavuhePRR"
      },
      "source": [
        "labels_to_show = []       # Labels of the images to show\n",
        "bounding_box_list = []    # List of the boudning boxes drawed.\n",
        "imgs_read_list = []       # List of the images read (no trasformations)\n",
        "imgs_list = []            # List of the images trasformated (with bouding boxes)\n",
        "title_images_list = []    # Label formattated.\n",
        "\n",
        "labels_to_show.append(random.randint(0, len(dirs) - 1))\n",
        "labels_to_show.append(random.randint(0, len(dirs) - 1))\n",
        "labels_to_show.append(random.randint(0, len(dirs) - 1))\n",
        "labels_to_show.append(random.randint(0, len(dirs) - 1))\n",
        "\n",
        "for i in range(0, 4):\n",
        "    alpha = 0.5\n",
        "    bounding_box_list.clear()\n",
        "\n",
        "    index_label = labels_to_show[i]\n",
        "    class_label = dataset[index_label]\n",
        "\n",
        "    imgs = class_label.img_list\n",
        "    annotations = class_label.annotation_list\n",
        "    label = class_label.label_ds\n",
        "\n",
        "    index_img = random.randint(0, len(imgs) - 1)\n",
        "\n",
        "    # Parse xml\n",
        "    tree = ET.parse(annotations[index_img])\n",
        "    root = tree.getroot()\n",
        "\n",
        "    # Get bounding boxes\n",
        "    bounding_box_list, _ = get_bounding_boxes(annotations[index_img])\n",
        "\n",
        "    # Image\n",
        "    img_read = cv.imread(imgs[index_img])\n",
        "\n",
        "    img_original = cv.resize(img_read, (400,400))\n",
        "    imgs_read_list.append(img_original)\n",
        "\n",
        "    # Mask\n",
        "    mask = np.zeros_like(img_read[:, :, 0])\n",
        "\n",
        "    for j, box in enumerate(bounding_box_list):\n",
        "        # Draw rectangle\n",
        "        cv.rectangle(img_read, convert_point_to_tuple(box[0]), convert_point_to_tuple(box[1]), (0, 0, 255), 5)\n",
        "\n",
        "        polygon = np.array([[bounding_box_list[j][0]], [[bounding_box_list[j][1][0],\n",
        "                            bounding_box_list[j][0][1]]], [bounding_box_list[j][1]],\n",
        "                            [[bounding_box_list[j][0][0], bounding_box_list[j][1][1]]]])\n",
        "        \n",
        "        cv.fillConvexPoly(mask, polygon, 1)\n",
        "\n",
        "    # Get polygon\n",
        "    img = cv.bitwise_and(img_read, img_read, mask=mask)\n",
        "    img = cv.addWeighted(img_read.copy(), alpha, img, 1 - alpha, 0)\n",
        "    img = cv.resize(img, (400, 400))\n",
        "    imgs_list.append(img)\n",
        "\n",
        "    title_images_list.append(class_label.label_name)\n",
        "\n",
        "\n",
        "imgs_stack = stack_images(1, ([draw_description(imgs_read_list[0], title_images_list[0]), draw_description(imgs_list[0], title_images_list[0])],\n",
        "                              [draw_description(imgs_read_list[1], title_images_list[1]), draw_description(imgs_list[1], title_images_list[1])],\n",
        "                              [draw_description(imgs_read_list[2], title_images_list[2]), draw_description(imgs_list[2], title_images_list[2])],\n",
        "                              [draw_description(imgs_read_list[3], title_images_list[3]), draw_description(imgs_list[3], title_images_list[3])]))\n",
        "\n",
        "cv2_imshow(imgs_stack)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj5hVuJpenUP"
      },
      "source": [
        "### Trainining, validation e test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz-Ok2DrfCUS"
      },
      "source": [
        "La funzione *split_dataset* suddivide il dataset in training, validation e test set rispettivamente al 70% (training), 20% (validation) e 10% (test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1paDilqfCdB"
      },
      "source": [
        "def split_dataset(dataset):\n",
        "    r\"\"\"\n",
        "    Slip dataset in training, validation and test set:\n",
        "        - 70% training set;\n",
        "        - 20% validation set:\n",
        "        - 10% test set.\n",
        "\n",
        "    :param dataset: dataset to split\n",
        "    :return : training, validation and test set.\n",
        "    \"\"\"\n",
        "    length_dataset = len(dataset)\n",
        "\n",
        "    length_train = np.int_(length_dataset * 0.7)\n",
        "    length_validate = np.int_(length_dataset * 0.2)\n",
        "\n",
        "    training_dataset = Subset(dataset, range(0, length_train))\n",
        "    validation_dataset = Subset(dataset, range(length_train, length_train + length_validate))\n",
        "\n",
        "    dataset.transofrms = get_transform(train=False)     # Disabled the transforms for test set\n",
        "    test_dataset = Subset(dataset, range(length_train + length_validate, len(dataset)))\n",
        "\n",
        "    return training_dataset, validation_dataset, test_dataset"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxr6AHDwfjOL"
      },
      "source": [
        "Partizionamento dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhHUsdM6e2bB",
        "outputId": "78d93fb0-d65c-459c-bc25-35cfcc436676"
      },
      "source": [
        "trainining_set = []\n",
        "validation_set = []\n",
        "test_set = []\n",
        "\n",
        "for _set in dataset:\n",
        "    \n",
        "    training_dataset, validation_dataset, test_dataset = split_dataset(_set)\n",
        "\n",
        "    trainining_set.append(training_dataset)\n",
        "    validation_set.append(validation_dataset)\n",
        "    test_set.append(test_dataset)\n",
        "\n",
        "training_dataset, validation_dataset, test_dataset = ConcatDataset(trainining_set), ConcatDataset(validation_set), \\\n",
        "                                                     ConcatDataset(test_set)\n",
        "\n",
        "print(f\"** Dataset caricato correttamente! Totale immagini: {len(training_dataset) + len(validation_dataset) + len(test_dataset)} **\\n\")\n",
        "print(f\"Dimensione training: {len(training_dataset)} immagini - (70%)\")\n",
        "print(f\"Dimensione validation: {len(validation_dataset)} immagini - (20%)\")\n",
        "print(f\"Dimensione test: {len(test_dataset)} immagini - (10%)\\n\\n\")\n",
        "print(f\"Totale immagini partizionate: {len(training_dataset) + len(validation_dataset) + len(test_dataset)}\\n\\n\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "** Dataset caricato correttamente! Totale immagini: 953 **\n",
            "\n",
            "Dimensione training: 657 immagini - (70%)\n",
            "Dimensione validation: 181 immagini - (20%)\n",
            "Dimensione test: 115 immagini - (10%)\n",
            "\n",
            "\n",
            "Totale immagini partizionate: 953\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ALhixEPKWuZ"
      },
      "source": [
        "Creazione del *DataLoader* rispettivamente per training, validation e test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6rwytfBKW0W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91b69722-5f26-4c89-a341-dcd0d26e498b"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "# Training set\n",
        "training_loader = DataLoader(training_dataset,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=True,\n",
        "                             num_workers=4,\n",
        "                             collate_fn=utils.collate_fn)\n",
        "\n",
        "# Validation set\n",
        "validation_loader = DataLoader(validation_dataset,\n",
        "                               batch_size=batch_size,\n",
        "                               shuffle=True,\n",
        "                               num_workers=4,\n",
        "                               collate_fn=utils.collate_fn)\n",
        "\n",
        "# Test set\n",
        "test_loader = DataLoader(test_dataset,\n",
        "                         batch_size=batch_size,\n",
        "                         shuffle=False,\n",
        "                         num_workers=4,\n",
        "                         collate_fn=utils.collate_fn)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCJjzRpNLf_4"
      },
      "source": [
        "## Modello della rete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o0M7MEWL6cH"
      },
      "source": [
        "La funzione *get_model* inizializza la rete neurale Faster R-CNN preaddestrata."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlazmflELjX4"
      },
      "source": [
        "def get_model(num_classes):\n",
        "  r\"\"\"\n",
        "  Model initialization.\n",
        "  :param num_classes: number of classes of the problem.\n",
        "  \"\"\"\n",
        "\n",
        "  # Load the model pretrained\n",
        "  model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "\n",
        "  # Get number of input features for the classifier\n",
        "  in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "  model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQt4vqCVMEqh"
      },
      "source": [
        "Modifica del modello aggiungendo una specifica backbone."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHSwKTQGMEwE"
      },
      "source": [
        "# backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
        "# backbone.out_channels = 1280\n",
        "\n",
        "# anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
        "#                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
        "\n",
        "# roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n",
        "#                                                output_size=7,\n",
        "#                                                sampling_ratio=2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtFA9lvcO-y5"
      },
      "source": [
        "Definizione del modello"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253,
          "referenced_widgets": [
            "62ad27ced64948018269bcb861dcb6e7",
            "5a80684b599f402d96cee66b8557b20b",
            "26a72f79058e4d43b8bf516b00918b45",
            "5822ce01016d437d9c42b15886b04425",
            "9df443915b8b41d2afba52d37096af47",
            "63c78dc882194af5a3d25d436bbac179",
            "3503869b20c748a28e2f79f05c0ace99",
            "3dc4264695374fbeb9f01c3b4acb14af"
          ]
        },
        "id": "evrPDmw_O-3u",
        "outputId": "c28351e9-e325-4bd4-d02f-97ff23e7ff73"
      },
      "source": [
        "# Classes\n",
        "num_classes = len(total_labels) + 1  # N classes + background\n",
        "\n",
        "# Model\n",
        "model = get_model(num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Runtime used: {device}\\n\\n\")\n",
        "model = model.to(device)\n",
        "\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(params, lr=0.005,\n",
        "                            momentum=0.9,\n",
        "                            weight_decay=0.0005)\n",
        "\n",
        "# learning rate withStep LR\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
        "                                               step_size=3,\n",
        "                                               gamma=0.1)\n",
        "\n",
        "# Epoch\n",
        "num_epochs = 5\n",
        "\n",
        "print(\"** SUMMARY TRAINING **\\n\")\n",
        "print(f\"- Classes of the problem: {num_classes};\")\n",
        "print(f\"- Epochs number: {num_epochs};\")\n",
        "print(F\"- Optimizer: {optimizer.__class__.__name__},\")\n",
        "print(f\"             Learning Rate: {optimizer.defaults['lr']},\")\n",
        "print(f\"             Momentum: {optimizer.defaults['momentum']}.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62ad27ced64948018269bcb861dcb6e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Runtime used: cuda\n",
            "\n",
            "\n",
            "** SUMMARY TRAINING **\n",
            "\n",
            "- Classes of the problem: 25;\n",
            "- Epochs number: 5;\n",
            "- Optimizer: SGD,\n",
            "             Learning Rate: 0.005,\n",
            "             Momentum: 0.9.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr4A39Y7U5Xk"
      },
      "source": [
        "## Addestramento"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPXYj2_a9yBg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "outputId": "0cfa7bfb-89d9-4095-d355-a4ff716dcb6c"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    # Training \n",
        "    train_one_epoch(model, optimizer, training_loader, device, epoch, print_freq=10)\n",
        "\n",
        "    # Update LR\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # Evaluation\n",
        "    evaluate(model, test_loader, device=device)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-f28e132144b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Update LR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup_lr_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarmup_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/utils.py\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    199\u001b[0m         ])\n\u001b[1;32m    200\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 257, in __getitem__\n    return self.datasets[dataset_idx][sample_idx]\n  File \"/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataset.py\", line 311, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"<ipython-input-49-a9f23594785b>\", line 83, in __getitem__\n    is_crowd = torch.zeros((len(bb_tensor),), dtype=torch.int64)\nTypeError: object of type 'NoneType' has no len()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXWnGD8eVK3f"
      },
      "source": [
        "## Risultati ottenuti"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfOtEXbGiw5E"
      },
      "source": [
        "Note:\n",
        "\n",
        "- Verificare se per il test set, le operazioni di data augmentation non vengono effettuate;\n",
        "\n",
        "- Valutare se aggiungere la modifica della backbone della rete;\n",
        "\n",
        "\n",
        "Errori:\n",
        "\n",
        "- bug di albumentations con i riquadri delle bb;\n",
        "\n",
        "- non riesco a creare bounding box per immagini senza oggetti (label free)\n",
        "\n",
        "- capire come allenare i 3 ds: traininf, validation e test (Ã¨ possibile usare piÃ¹ volte la funzione di pytorch ??)"
      ]
    }
  ]
}